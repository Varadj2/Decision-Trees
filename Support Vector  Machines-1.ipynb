{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d4591ef7-22ae-4d9c-879a-439fe35041a2",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5214177-2670-4259-a592-b4f29abc6ccd",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) involves finding the hyperplane that separates the classes while maximizing the margin between the closest data points (support vectors) from each class. For a linearly separable dataset, the decision boundary is a hyperplane defined by the equation:\n",
    "\n",
    "\\[ w^T x + b = 0 \\]\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the input vector (a feature vector).\n",
    "- \\( w \\) is the weight vector (coefficients) perpendicular to the hyperplane.\n",
    "- \\( b \\) is the bias term (intercept).\n",
    "- \\( w^T \\) denotes the transpose of the weight vector.\n",
    "\n",
    "The decision boundary divides the feature space into two regions corresponding to the two classes. Points on one side of the hyperplane are classified as one class, while points on the other side are classified as the other class.\n",
    "\n",
    "The distance between the hyperplane and the closest data point (support vector) from each class is the margin, denoted as \\( \\frac{1}{\\|w\\|} \\). The goal of SVM is to maximize this margin while minimizing the classification error.\n",
    "\n",
    "Mathematically, for a linearly separable dataset, the objective function of a linear SVM can be written as:\n",
    "\n",
    "\\[ \\min_{w,b} \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\quad \\text{for } i = 1, 2, ..., m \\]\n",
    "\n",
    "where:\n",
    "- \\( (x^{(i)}, y^{(i)}) \\) are the training examples.\n",
    "- \\( y^{(i)} \\) is the class label (+1 or -1).\n",
    "- \\( m \\) is the number of training examples.\n",
    "- \\( \\|w\\| \\) is the Euclidean norm (magnitude) of the weight vector \\( w \\).\n",
    "\n",
    "The optimization problem is solved using techniques like quadratic programming to find the optimal \\( w \\) and \\( b \\) that satisfy the constraints and maximize the margin.\n",
    "\n",
    "If the dataset is not linearly separable, a soft-margin SVM is used, which introduces a slack variable \\( \\xi^{(i)} \\) for each training example to allow for some misclassifications. The objective function then becomes:\n",
    "\n",
    "\\[ \\min_{w,b,\\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\xi^{(i)} \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y^{(i)}(w^T x^{(i)} + b) \\geq 1 - \\xi^{(i)} \\quad \\text{and} \\quad \\xi^{(i)} \\geq 0 \\]\n",
    "\n",
    "where \\( C \\) is the regularization parameter, controlling the trade-off between maximizing the margin and minimizing the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2202a-3a7e-4fab-8195-4370070b0cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "92956151-e186-43d0-8dae-4f55cee90579",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44655a-87ed-473d-90bd-7db23a103052",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) aims to maximize the margin between the decision boundary (hyperplane) and the closest data points (support vectors) while minimizing the classification error. Mathematically, the objective function of a linear SVM for a dataset with \\( m \\) training examples is formulated as follows:\n",
    "\n",
    "\\[ \\min_{w,b} \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\]\n",
    "\n",
    "where:\n",
    "- \\( w \\) is the weight vector (coefficients) perpendicular to the hyperplane.\n",
    "- \\( b \\) is the bias term (intercept).\n",
    "- \\( x^{(i)} \\) is the feature vector of the \\( i \\)-th training example.\n",
    "- \\( y^{(i)} \\) is the class label of the \\( i \\)-th training example (\\( y^{(i)} = 1 \\) for positive class, \\( y^{(i)} = -1 \\) for negative class).\n",
    "- \\( \\|w\\|^2 \\) is the squared Euclidean norm (magnitude) of the weight vector \\( w \\), which represents the margin.\n",
    "  \n",
    "The objective function seeks to minimize \\( \\frac{1}{2} \\|w\\|^2 \\), which is equivalent to maximizing the margin between the decision boundary and the closest data points. The factor \\( \\frac{1}{2} \\) is included for mathematical convenience, as it simplifies the derivative of the objective function.\n",
    "\n",
    "The constraints \\( y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\) ensure that all data points are correctly classified and are at least at a distance of \\( \\frac{1}{\\|w\\|} \\) from the decision boundary. \n",
    "\n",
    "In summary, the objective function of a linear SVM aims to find the optimal hyperplane that maximizes the margin between classes, ensuring good generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bae19d-921d-4a08-bbc0-bc8f035589de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "682a0e1c-0b07-482e-8b3f-df87e9810822",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce255c73-a304-4913-96c4-865e1bf63b7b",
   "metadata": {},
   "source": [
    "The kernel trick is a method used in Support Vector Machines (SVMs) to implicitly map data into a higher-dimensional feature space without explicitly computing the transformation. This allows SVMs to efficiently handle non-linearly separable data by effectively finding non-linear decision boundaries in the original input space.\n",
    "\n",
    "In a traditional SVM with a linear kernel, the decision boundary is a hyperplane defined by a linear combination of input features. However, many real-world datasets are not linearly separable in their original feature space. The kernel trick addresses this limitation by introducing a kernel function that computes the dot product of data points in a higher-dimensional space, allowing SVMs to learn complex decision boundaries.\n",
    "\n",
    "Mathematically, the kernel trick is based on the Mercer's theorem, which states that a symmetric function \\( K(x, x') \\) can be used as a valid kernel if and only if the corresponding Gram matrix \\( K \\), defined as \\( K_{ij} = K(x^{(i)}, x^{(j)}) \\), is positive semi-definite for any set of points \\( x^{(1)}, x^{(2)}, ..., x^{(m)} \\) in the input space.\n",
    "\n",
    "There are several commonly used kernel functions, each suitable for different types of data:\n",
    "\n",
    "1. **Linear Kernel (no kernel trick)**:\n",
    "   \\[ K(x, x') = x^T x' \\]\n",
    "   This is the standard inner product of the input features.\n",
    "\n",
    "2. **Polynomial Kernel**:\n",
    "   \\[ K(x, x') = (x^T x' + c)^d \\]\n",
    "   This kernel maps the data into a higher-dimensional space using polynomial functions.\n",
    "\n",
    "3. **Gaussian Radial Basis Function (RBF) Kernel**:\n",
    "   \\[ K(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right) \\]\n",
    "   This kernel maps the data into an infinite-dimensional space using a Gaussian function.\n",
    "\n",
    "4. **Sigmoid Kernel**:\n",
    "   \\[ K(x, x') = \\tanh(\\alpha x^T x' + c) \\]\n",
    "   This kernel maps the data into a higher-dimensional space using hyperbolic tangent functions.\n",
    "\n",
    "The kernel trick allows SVMs to find complex decision boundaries in the original input space without explicitly computing the transformations, thus avoiding the computational burden of working in high-dimensional spaces. It provides a powerful tool for handling non-linear relationships in data, making SVMs versatile for various machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22c015-0da2-4710-8961-d44aa3875dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a1a1581d-3880-4e05-8c96-6fd7af25a377",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac95d63-fa5d-434a-8fc0-b07bcd3e63b6",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVMs), support vectors are the data points that lie closest to the decision boundary (hyperplane) and play a crucial role in defining the decision boundary. These are the points that have a non-zero value for the Lagrange multiplier (alpha) in the optimization problem. Support vectors are the key elements that determine the position and orientation of the decision boundary, and the entire SVM model is essentially defined by these support vectors.\n",
    "\n",
    "Here's an explanation of the role of support vectors with an example:\n",
    "\n",
    "Consider a simple binary classification problem where we have two classes, represented by red and blue points in a 2D feature space. We want to find a decision boundary (hyperplane) that separates the two classes. \n",
    "\n",
    "![SVM Example](https://i.imgur.com/zoRsiNB.png)\n",
    "\n",
    "In the above image, the red and blue points represent the two classes, and the decision boundary is the dashed line. The two classes are not linearly separable in the original feature space.\n",
    "\n",
    "When we train an SVM on this dataset, the SVM algorithm will find the optimal decision boundary that maximizes the margin between the classes. This decision boundary will be defined by a subset of data points known as support vectors.\n",
    "\n",
    "In this example, the support vectors are the points marked by larger circles. These are the points that are closest to the decision boundary. They determine the position and orientation of the decision boundary because their distance to the boundary directly affects the margin.\n",
    "\n",
    "For example:\n",
    "- The support vectors marked in red are closest to the blue points, and vice versa.\n",
    "- Changing the position of any other points that are not support vectors will not affect the decision boundary as long as they remain on the correct side of the margin.\n",
    "\n",
    "![SVM Example with Support Vectors](https://i.imgur.com/gX2Wp9m.png)\n",
    "\n",
    "In the above image, the dashed line represents the decision boundary, and the solid lines represent the margin. The support vectors are the points lying on the margin lines.\n",
    "\n",
    "The significance of support vectors in SVM can be summarized as follows:\n",
    "1. **Determining the Decision Boundary**: Support vectors are the critical points that define the decision boundary. The decision boundary is constructed by maximizing the margin around these support vectors.\n",
    "2. **Robustness to Outliers**: Since the decision boundary depends only on the support vectors, SVMs are robust to outliers or noise in the dataset that doesn't affect the support vectors.\n",
    "3. **Efficiency**: The computational complexity of SVM depends on the number of support vectors rather than the entire dataset, making SVM efficient in high-dimensional spaces or with large datasets.\n",
    "\n",
    "In summary, support vectors are the backbone of SVMs, crucial for defining the decision boundary and ensuring the robustness and efficiency of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff544559-be67-44e2-ae03-b0192ec6fc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5dcc5965-bd96-4e89-8f74-85692164e7d2",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in \n",
    "SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1d4e3-e0f3-4661-86af-a1fd52d156fb",
   "metadata": {},
   "source": [
    "Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM) with examples and graphs.\n",
    "\n",
    "**1. Hyperplane**:\n",
    "The hyperplane is the decision boundary that separates the classes in an SVM. In a binary classification problem, it's a flat subspace of dimension \\(n-1\\), where \\(n\\) is the number of features. In 2D, it's a line, and in 3D, it's a plane. The hyperplane is defined by the equation \\(w^Tx + b = 0\\), where \\(w\\) is the weight vector, \\(x\\) is the input vector, and \\(b\\) is the bias.\n",
    "\n",
    "**2. Marginal Plane**:\n",
    "The marginal plane is the area that lies parallel to the hyperplane and is equidistant from it. In a hard-margin SVM, the margin is the distance between the hyperplane and the closest data point (support vector) from each class. In a soft-margin SVM, the margin is widened to allow for some misclassifications.\n",
    "\n",
    "**3. Soft Margin**:\n",
    "In a soft-margin SVM, some misclassifications are allowed to achieve a wider margin and better generalization to unseen data. This is useful when the data is not perfectly separable. The soft-margin SVM introduces slack variables (\\(\\xi\\)) to penalize misclassifications. The objective function is modified to minimize the misclassification errors while maximizing the margin.\n",
    "\n",
    "**4. Hard Margin**:\n",
    "In a hard-margin SVM, no misclassifications are allowed, and the margin is maximized without any violations. This is suitable for linearly separable data. Hard-margin SVMs are more sensitive to outliers and noise in the data.\n",
    "\n",
    "Let's illustrate these concepts with examples and graphs:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.array([[1, 2], [2, 3], [3, 3], [6, 7], [7, 8], [8, 9]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])  # 0: Class 1, 1: Class 2\n",
    "\n",
    "# Plot data points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=100)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot hyperplane\n",
    "w = np.array([1, -1])  # Example weight vector\n",
    "b = -2  # Example bias\n",
    "x_hyper = np.linspace(0, 10, 100)\n",
    "y_hyper = (-w[0] * x_hyper - b) / w[1]\n",
    "plt.plot(x_hyper, y_hyper, 'k--', label='Hyperplane')\n",
    "\n",
    "# Plot marginal planes (soft margin)\n",
    "margin = 1  # Margin distance\n",
    "plt.plot(x_hyper, y_hyper + margin, 'r--', label='Soft Margin')\n",
    "plt.plot(x_hyper, y_hyper - margin, 'r--')\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter([3, 7], [3, 7], c='k', marker='x', s=200, label='Support Vectors')\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.legend()\n",
    "plt.title('SVM with Hyperplane, Marginal Plane (Soft Margin)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We have two classes, represented by red and blue points.\n",
    "- The hyperplane (decision boundary) separates the two classes.\n",
    "- The soft margin is depicted by the dashed red lines, representing the area equidistant from the hyperplane.\n",
    "- Support vectors are the points lying on the margin lines.\n",
    "\n",
    "The graph illustrates the concepts of hyperplane, marginal plane (soft margin), and support vectors in SVM.\n",
    "\n",
    "Let's modify this example to illustrate a hard-margin SVM as well.\n",
    "\n",
    "Here's the modified example to illustrate a hard-margin SVM:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.array([[1, 2], [2, 3], [3, 3], [6, 7], [7, 8], [8, 9]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])  # 0: Class 1, 1: Class 2\n",
    "\n",
    "# Plot data points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=100)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot hyperplane\n",
    "w = np.array([1, -1])  # Example weight vector\n",
    "b = -2  # Example bias\n",
    "x_hyper = np.linspace(0, 10, 100)\n",
    "y_hyper = (-w[0] * x_hyper - b) / w[1]\n",
    "plt.plot(x_hyper, y_hyper, 'k--', label='Hyperplane')\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter([3, 7], [3, 7], c='k', marker='x', s=200, label='Support Vectors')\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.legend()\n",
    "plt.title('SVM with Hyperplane and Support Vectors (Hard Margin)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We have removed the soft margin lines.\n",
    "- The decision boundary (hyperplane) now directly touches the closest data points (support vectors) from each class, creating a hard margin.\n",
    "- No misclassifications are allowed in a hard-margin SVM, so the decision boundary is determined solely by the support vectors.\n",
    "\n",
    "The graph illustrates the concept of a hard-margin SVM with a hyperplane and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec148e01-2128-48ba-a9a7-de5f52c1079c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "bc30c700-e7d0-496b-b0b6-27ab1dc7ef81",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its \n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of \n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff125883-50be-431e-a52e-ead4cdc86806",
   "metadata": {},
   "source": [
    "Sure, let's implement a linear SVM classifier on the Iris dataset using both scikit-learn and from scratch. We'll compare their performances and plot the decision boundaries.\n",
    "\n",
    "First, let's load the Iris dataset, split it into training and testing sets, and import necessary libraries.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "Now, let's implement a linear SVM classifier using scikit-learn and train it on the training set.\n",
    "\n",
    "```python\n",
    "# Train a linear SVM classifier using scikit-learn\n",
    "svm_clf_sklearn = SVC(kernel='linear')\n",
    "svm_clf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred_sklearn = svm_clf_sklearn.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(\"Accuracy of scikit-learn SVM:\", accuracy_sklearn)\n",
    "```\n",
    "\n",
    "Next, let's plot the decision boundaries of the trained model using two features.\n",
    "\n",
    "```python\n",
    "# Plot decision boundaries\n",
    "def plot_decision_boundary(X, y, model, feature1, feature2):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[:, feature1], X[:, feature2], c=y, cmap=plt.cm.Paired, s=100)\n",
    "\n",
    "    # Create meshgrid of feature values\n",
    "    x_min, x_max = X[:, feature1].min() - 1, X[:, feature1].max() + 1\n",
    "    y_min, y_max = X[:, feature2].min() - 1, X[:, feature2].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    # Predict the labels for each point in the meshgrid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[feature1])\n",
    "    plt.ylabel(iris.feature_names[feature2])\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_train, y_train, svm_clf_sklearn, 0, 1)\n",
    "```\n",
    "\n",
    "Now, let's implement a linear SVM classifier from scratch using Python. We'll use the Sequential Minimal Optimization (SMO) algorithm for optimization.\n",
    "\n",
    "```python\n",
    "class LinearSVM:\n",
    "    def __init__(self, C=1.0, max_iter=100, tol=1e-3):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(n_samples):\n",
    "                E_i = self.decision_function(X[i]) - y[i]\n",
    "\n",
    "                if (y[i] * E_i < -self.tol and self.W[i] < self.C) or (y[i] * E_i > self.tol and self.W[i] > 0):\n",
    "                    j = np.random.choice(np.delete(np.arange(n_samples), i))\n",
    "                    E_j = self.decision_function(X[j]) - y[j]\n",
    "\n",
    "                    W_i_old = self.W[i]\n",
    "                    W_j_old = self.W[j]\n",
    "\n",
    "                    if y[i] != y[j]:\n",
    "                        L = max(0, self.W[j] - self.W[i])\n",
    "                        H = min(self.C, self.C + self.W[j] - self.W[i])\n",
    "                    else:\n",
    "                        L = max(0, self.W[i] + self.W[j] - self.C)\n",
    "                        H = min(self.C, self.W[i] + self.W[j])\n",
    "\n",
    "                    if L == H:\n",
    "                        continue\n",
    "\n",
    "                    eta = 2 * X[i].dot(X[j]) - X[i].dot(X[i]) - X[j].dot(X[j])\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "\n",
    "                    self.W[j] = self.W[j] - y[j] * (E_i - E_j) / eta\n",
    "                    self.W[j] = min(H, max(L, self.W[j]))\n",
    "\n",
    "                    if abs(self.W[j] - W_j_old) < 1e-5:\n",
    "                        continue\n",
    "\n",
    "                    self.W[i] = self.W[i] + y[i] * y[j] * (W_j_old - self.W[j])\n",
    "\n",
    "                    b1 = self.b - E_i - y[i] * (self.W[i] - W_i_old) * X[i].dot(X[i]) - y[j] * (self.W[j] - W_j_old) * X[i].dot(X[j])\n",
    "                    b2 = self.b - E_j - y[i] * (self.W[i] - W_i_old) * X[i].dot(X[j]) - y[j] * (self.W[j] - W_j_old) * X[j].dot(X[j])\n",
    "                    if 0 < self.W[i] < self.C:\n",
    "                        self.b = b1\n",
    "                    elif 0 < self.W[j] < self.C:\n",
    "                        self.b = b2\n",
    "                    else:\n",
    "                        self.b = (b1 + b2) / 2\n",
    "\n",
    "                    num_changed_alphas += 1\n",
    "\n",
    "            if num_changed_alphas == 0:\n",
    "                break\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.W) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.decision_function(X))\n",
    "\n",
    "# Train a linear SVM classifier from scratch\n",
    "svm_clf_scratch = LinearSVM()\n",
    "svm_clf_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred_scratch = svm_clf_scratch.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(\"Accuracy of SVM from scratch:\", accuracy_scratch)\n",
    "\n",
    "# Plot decision boundary for SVM from scratch\n",
    "plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca214cee-8f7f-4ee6-ab09-87f23e089123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
